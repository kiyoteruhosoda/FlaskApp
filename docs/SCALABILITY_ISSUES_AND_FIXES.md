# å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å•é¡Œã¨ä¿®æ­£

## ğŸš¨ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ

### 1. ãƒ­ã‚°ã® `details` ã‚«ãƒ©ãƒ ï¼ˆJSONï¼‰ã¸ã®å¤§é‡ãƒ‡ãƒ¼ã‚¿è“„ç©ãƒªã‚¹ã‚¯

#### å•é¡Œã®æ‰€åœ¨

**ç¾åœ¨ã®å®Ÿè£…**:
```python
# use_case.py (Line 241-257)
log_performance(
    "local_import_task",
    duration_ms,
    session_id=result.session_id,
    celery_task_id=celery_task_id,
    total_files=result.processed,      # â† æ•°ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚å•é¡Œãªã—
    success_count=result.success,      # â† æ•°ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚å•é¡Œãªã—
    failed_count=result.failed,        # â† æ•°ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚å•é¡Œãªã—
)
```

**æ½œåœ¨çš„ãªå•é¡Œ**:
- `result` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã« **å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚„ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ** ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§
- `details` JSONã«é…åˆ—ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã¨ **æ•°ä¸‡ã€œæ•°åä¸‡ä»¶ã§æ¡æº¢ã‚Œ**
- ä¾‹: 10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ« Ã— å¹³å‡200ãƒã‚¤ãƒˆ/ãƒ‘ã‚¹ = 20MB ã®JSON

#### å½±éŸ¿ç¯„å›²

| å ´æ‰€ | ãƒªã‚¹ã‚¯ | å½±éŸ¿ |
|------|--------|------|
| **use_case.py** | `result.errors` ãŒå…¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€å ´åˆ | JSONè‚¥å¤§åŒ– |
| **file_importer.py** | å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚°ãªã®ã§å•é¡Œãªã— âœ… | ãªã— |
| **TroubleshootingEngine** | `recommended_actions` ã¯å›ºå®š5-10ä»¶ âœ… | ãªã— |
| **AuditLogger** | `details` ã«ä»»æ„ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å…¥ã‚Œ | JSONè‚¥å¤§åŒ– |

---

### 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®åˆ¶ç´„

#### ç¾åœ¨ã®ã‚¹ã‚­ãƒ¼ãƒ

```sql
-- migrations/versions/a1b2c3d4e5f6_add_local_import_audit_log.py
sa.Column('details', sa.JSON(), nullable=True),              -- âš ï¸ ã‚µã‚¤ã‚ºåˆ¶é™ãªã—
sa.Column('recommended_actions', sa.JSON(), nullable=True),  -- âœ… å›ºå®šãƒªã‚¹ãƒˆ
```

**MariaDB/MySQLã®åˆ¶ç´„**:
- JSONå‹ã®æœ€å¤§ã‚µã‚¤ã‚º: **16MB** (max_allowed_packet)
- ç¾å®Ÿçš„ãªæ¨å¥¨ã‚µã‚¤ã‚º: **1MBæœªæº€**
- è¶…éæ™‚: `Packet for query is too large` ã‚¨ãƒ©ãƒ¼

---

### 3. ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã®å•é¡Œ

#### ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è“„ç©

```python
# ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ï¼ˆæ¨æ¸¬ï¼‰
result = TaskResult(
    processed=len(all_files),        # âœ… æ•°å€¤ã®ã¿
    success=len(success_files),      # âœ… æ•°å€¤ã®ã¿
    failed=len(failed_files),        # âœ… æ•°å€¤ã®ã¿
    errors=[...]                     # âš ï¸ å…¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼Ÿ
)
```

**å•é¡Œ**:
- 10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†çµæœã‚’ **ãƒ¡ãƒ¢ãƒªã«ä¿æŒ**
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…åˆ—ãŒè‚¥å¤§åŒ–
- Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒ¡ãƒ¢ãƒªä¸è¶³

---

## âœ… ä¿®æ­£æ–¹é‡

### åŸå‰‡

1. **é›†è¨ˆå€¤ã®ã¿ã‚’JSONã«ä¿å­˜**
   - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®é…åˆ—ã¯ä¿å­˜ã—ãªã„
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ **ä»£è¡¨ä¾‹ã®ã¿** ã¾ãŸã¯ **åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«**

2. **è©³ç´°ãƒ‡ãƒ¼ã‚¿ã¯åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜**
   - å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ã‚°ã¯ `item_id` ã§ç´ä»˜ã‘
   - å¤§é‡ãƒ‡ãƒ¼ã‚¿ã¯åœ§ç¸®ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

3. **ã‚µã‚¤ã‚ºåˆ¶é™ã‚’ã‚³ãƒ¼ãƒ‰ã§å¼·åˆ¶**
   - JSONä¿å­˜å‰ã« **1MBåˆ¶é™** ã‚’ãƒã‚§ãƒƒã‚¯
   - è¶…éæ™‚ã¯åˆ‡ã‚Šè©°ã‚ã¦ã‚µãƒãƒªãƒ¼åŒ–

---

## ğŸ”§ å…·ä½“çš„ãªä¿®æ­£

### ä¿®æ­£1: AuditLoggerã«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `features/photonest/infrastructure/local_import/audit_logger.py`

```python
import json

class AuditLogger:
    """ç›£æŸ»ãƒ­ã‚°è¨˜éŒ²å™¨"""
    
    MAX_DETAILS_SIZE_BYTES = 900_000  # 900KBï¼ˆä½™è£•ã‚’æŒã£ã¦1MBæœªæº€ï¼‰
    MAX_ACTIONS_COUNT = 50            # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æœ€å¤§50ä»¶
    
    def log(self, entry: AuditLogEntry) -> None:
        """ãƒ­ã‚°ã‚’è¨˜éŒ²ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰"""
        
        # 1. detailsã®ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        entry.details = self._truncate_details(entry.details)
        
        # 2. recommended_actionsã®ä»¶æ•°åˆ¶é™
        if len(entry.recommended_actions) > self.MAX_ACTIONS_COUNT:
            entry.recommended_actions = entry.recommended_actions[:self.MAX_ACTIONS_COUNT]
            entry.recommended_actions.append(
                f"ï¼ˆ{len(entry.recommended_actions) - self.MAX_ACTIONS_COUNT}ä»¶çœç•¥ï¼‰"
            )
        
        # æ—¢å­˜ã®ä¿å­˜å‡¦ç†
        try:
            self._repo.save(entry)
        except Exception as e:
            logger.error(f"ãƒ­ã‚°ã®DBä¿å­˜ã«å¤±æ•—: {e}", exc_info=True)
    
    def _truncate_details(self, details: dict) -> dict:
        """detailsã‚’åˆ‡ã‚Šè©°ã‚
        
        Args:
            details: å…ƒã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            dict: åˆ‡ã‚Šè©°ã‚ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆ900KBä»¥å†…ï¼‰
        """
        if not details:
            return details
        
        # JSONæ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦ã‚µã‚¤ã‚ºç¢ºèª
        json_str = json.dumps(details, ensure_ascii=False)
        size_bytes = len(json_str.encode('utf-8'))
        
        if size_bytes <= self.MAX_DETAILS_SIZE_BYTES:
            return details  # ã‚µã‚¤ã‚ºOK
        
        # ã‚µã‚¤ã‚ºè¶…é: é…åˆ—ã‚’åˆ‡ã‚Šè©°ã‚
        truncated = {}
        for key, value in details.items():
            if isinstance(value, list) and len(value) > 10:
                # é…åˆ—ã¯æœ€åˆã®5ä»¶ã¨æœ€å¾Œã®5ä»¶ã®ã¿ä¿å­˜
                truncated[key] = {
                    "_truncated": True,
                    "_original_count": len(value),
                    "first_items": value[:5],
                    "last_items": value[-5:],
                }
            else:
                truncated[key] = value
        
        # å†åº¦ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        json_str = json.dumps(truncated, ensure_ascii=False)
        size_bytes = len(json_str.encode('utf-8'))
        
        if size_bytes > self.MAX_DETAILS_SIZE_BYTES:
            # ãã‚Œã§ã‚‚è¶…éã™ã‚‹å ´åˆã¯ã‚µãƒãƒªãƒ¼ã®ã¿
            return {
                "_truncated": True,
                "_reason": "ã‚µã‚¤ã‚ºè¶…éã«ã‚ˆã‚Šè©³ç´°ã‚’çœç•¥",
                "_original_size_bytes": size_bytes,
                "keys": list(details.keys()),
            }
        
        return truncated
```

---

### ä¿®æ­£2: use_case.py ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°ã‚’ä¿®æ­£

**ãƒ•ã‚¡ã‚¤ãƒ«**: `features/photonest/application/local_import/use_case.py`

```python
# ä¿®æ­£å‰ï¼ˆLine 241-257ï¼‰
log_performance(
    "local_import_task",
    duration_ms,
    session_id=result.session_id,
    celery_task_id=celery_task_id,
    total_files=result.processed,
    success_count=result.success,
    failed_count=result.failed,
)

# ä¿®æ­£å¾Œ
log_performance(
    "local_import_task",
    duration_ms,
    session_id=result.session_id,
    celery_task_id=celery_task_id,
    total_files=result.processed,
    success_count=result.success,
    failed_count=result.failed,
    # âŒ å‰Šé™¤: errors=result.errorsï¼ˆå…¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
    # âœ… è¿½åŠ : ä»£è¡¨çš„ãªã‚¨ãƒ©ãƒ¼ã®ã¿
    sample_errors=result.errors[:5] if hasattr(result, 'errors') and result.errors else [],
    error_summary={
        "total_errors": len(result.errors) if hasattr(result, 'errors') else 0,
        "error_types": self._summarize_error_types(result.errors) if hasattr(result, 'errors') else {},
    },
)

# æ–°è¦ãƒ¡ã‚½ãƒƒãƒ‰
def _summarize_error_types(self, errors: list) -> dict:
    """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’é›†è¨ˆ
    
    Args:
        errors: ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®ä»¶æ•° {"FileNotFoundError": 10, ...}
    """
    from collections import Counter
    
    if not errors:
        return {}
    
    # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡º
    error_types = []
    for error in errors:
        if isinstance(error, dict) and "type" in error:
            error_types.append(error["type"])
        elif isinstance(error, Exception):
            error_types.append(type(error).__name__)
        else:
            error_types.append("Unknown")
    
    # ä»¶æ•°é›†è¨ˆ
    return dict(Counter(error_types))
```

---

### ä¿®æ­£3: file_importer.py ã¯æ—¢ã«å®‰å…¨ âœ…

**ç¾åœ¨ã®å®Ÿè£…**:
```python
# file_importer.py (Line 267-310)
log_performance(
    "file_import_success",
    duration_ms,
    session_id=session_id,
    item_id=item_id,
    file_size_bytes=file_size,  # â† å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã®ã¿
)
```

**è©•ä¾¡**: 
- âœ… å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®ãƒ­ã‚°ãªã®ã§å•é¡Œãªã—
- âœ… `file_size_bytes` ã¯æ•°å€¤1ã¤ã®ã¿
- âœ… ä¿®æ­£ä¸è¦

---

### ä¿®æ­£4: ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `migrations/versions/a1b2c3d4e5f6_add_local_import_audit_log.py`

```python
# è©³ç´°æƒ…å ±ï¼ˆJSONï¼‰
# æ³¨æ„: 1MBä»¥ä¸‹ã«åˆ¶é™ã™ã‚‹ã“ã¨ï¼ˆAuditLoggerå´ã§å¼·åˆ¶ï¼‰
sa.Column('details', sa.JSON(), nullable=True),

# æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆJSONé…åˆ—ï¼‰
# æ³¨æ„: æœ€å¤§50ä»¶ã«åˆ¶é™ã™ã‚‹ã“ã¨ï¼ˆAuditLoggerå´ã§å¼·åˆ¶ï¼‰
sa.Column('recommended_actions', sa.JSON(), nullable=True),
```

---

## ğŸ“Š ä¿®æ­£å¾Œã®åŠ¹æœ

### Beforeï¼ˆä¿®æ­£å‰ï¼‰

```
10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚:
  - detailsã‚µã‚¤ã‚º: 20MBï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä»¶ï¼‰
  - DBä¿å­˜: âŒ å¤±æ•—ï¼ˆPacket too largeï¼‰
  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨: 300MB+
```

### Afterï¼ˆä¿®æ­£å¾Œï¼‰

```
10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚:
  - detailsã‚µã‚¤ã‚º: 500KBï¼ˆã‚µãƒãƒªãƒ¼ã®ã¿ï¼‰
  - DBä¿å­˜: âœ… æˆåŠŸï¼ˆ900KBä»¥å†…ï¼‰
  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨: 50MB
```

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆæ–¹æ³•

### 1. ã‚µã‚¤ã‚ºåˆ¶é™ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```python
# tests/test_audit_logger_scalability.py

def test_truncate_large_details():
    """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã¯åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã‚‹"""
    logger = AuditLogger(mock_repo)
    
    # 10ä¸‡ä»¶ã®é…åˆ—ã‚’ä½œæˆ
    large_details = {
        "file_paths": [f"/path/to/file_{i}.jpg" for i in range(100_000)],
    }
    
    entry = AuditLogEntry(
        message="ãƒ†ã‚¹ãƒˆ",
        details=large_details,
    )
    
    logger.log(entry)
    
    # åˆ‡ã‚Šè©°ã‚ã‚‰ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
    saved_entry = mock_repo.last_saved_entry
    assert saved_entry.details["file_paths"]["_truncated"] is True
    assert saved_entry.details["file_paths"]["_original_count"] == 100_000
    assert len(saved_entry.details["file_paths"]["first_items"]) == 5


def test_json_size_limit():
    """JSONã‚µã‚¤ã‚ºãŒ900KBä»¥å†…ã«åˆ¶é™ã•ã‚Œã‚‹"""
    logger = AuditLogger(mock_repo)
    
    # 2MBã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    large_details = {
        "data": "x" * 2_000_000,
    }
    
    entry = AuditLogEntry(
        message="ãƒ†ã‚¹ãƒˆ",
        details=large_details,
    )
    
    logger.log(entry)
    
    # ã‚µã‚¤ã‚ºã‚’ç¢ºèª
    saved_entry = mock_repo.last_saved_entry
    json_str = json.dumps(saved_entry.details, ensure_ascii=False)
    size_bytes = len(json_str.encode('utf-8'))
    
    assert size_bytes < 900_000
```

### 2. å®Ÿéš›ã®å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ

```python
# tests/test_large_scale_import.py

def test_10k_files_import():
    """1ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    # 1ä¸‡å€‹ã®ãƒ€ãƒŸãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    test_files = create_test_files(count=10_000)
    
    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ
    result = use_case.execute(
        zip_path=test_zip,
        session_id=session_id,
    )
    
    # ãƒ­ã‚°ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª
    logs = db.session.query(LocalImportAuditLog).filter_by(
        session_id=session_id
    ).all()
    
    # å…¨ãƒ­ã‚°ã®detailsã‚µã‚¤ã‚ºã‚’ç¢ºèª
    for log in logs:
        if log.details:
            json_str = json.dumps(log.details, ensure_ascii=False)
            size_bytes = len(json_str.encode('utf-8'))
            assert size_bytes < 1_000_000, f"ãƒ­ã‚°ID {log.id} ãŒã‚µã‚¤ã‚ºè¶…é: {size_bytes} bytes"
```

---

## ğŸ“‹ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å®Ÿè£…å‰

- [ ] `AuditLogger._truncate_details()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
- [ ] `AuditLogger.MAX_DETAILS_SIZE_BYTES` å®šæ•°ã‚’å®šç¾©
- [ ] `use_case.py` ã®ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ã‚µãƒãƒªãƒ¼åŒ–
- [ ] `_summarize_error_types()` ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ 

### ãƒ†ã‚¹ãƒˆ

- [ ] `test_truncate_large_details()` ã‚’å®Ÿè£…
- [ ] `test_json_size_limit()` ã‚’å®Ÿè£…
- [ ] 1ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®çµ±åˆãƒ†ã‚¹ãƒˆ
- [ ] 10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ

### ãƒ‡ãƒ—ãƒ­ã‚¤

- [ ] ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã§å¤§é‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ
- [ ] ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
- [ ] æ—¢å­˜ãƒ­ã‚°ã®ã‚µã‚¤ã‚ºç¢ºèª

---

## ğŸ¯ ã¾ã¨ã‚

### ä¸»ãªå•é¡Œ

1. **JSONã‚«ãƒ©ãƒ ã«å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©ã™ã‚‹ã¨æ¡æº¢ã‚Œ** â†’ 900KBåˆ¶é™ã‚’è¿½åŠ 
2. **å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜** â†’ ã‚µãƒãƒªãƒ¼ã®ã¿ã«ä¿®æ­£
3. **é…åˆ—ã®ç„¡åˆ¶é™ä¿å­˜** â†’ å…ˆé ­/æœ«å°¾ã®ã¿ä¿å­˜

### ä¿®æ­£ã®å„ªå…ˆåº¦

| å„ªå…ˆåº¦ | ä¿®æ­£å†…å®¹ | ç†ç”± |
|--------|----------|------|
| **ğŸ”´ é«˜** | `AuditLogger._truncate_details()` è¿½åŠ  | DBä¿å­˜å¤±æ•—ã‚’é˜²æ­¢ |
| **ğŸŸ¡ ä¸­** | `use_case.py` ã®ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼åŒ– | ãƒ¡ãƒ¢ãƒªç¯€ç´„ |
| **ğŸŸ¢ ä½** | ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ  | å°†æ¥ã®ä¿å®ˆæ€§å‘ä¸Š |

### æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. ã¾ãš `AuditLogger` ã«ã‚µã‚¤ã‚ºåˆ¶é™ã‚’è¿½åŠ ï¼ˆ**å¿…é ˆ**ï¼‰
2. æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¯å¾ã€…ã«ä¿®æ­£ï¼ˆå¾Œæ–¹äº’æ›æ€§ç¶­æŒï¼‰
3. å¤§è¦æ¨¡ãƒ†ã‚¹ãƒˆã§å‹•ä½œç¢ºèª

**ä¿®æ­£ã—ãªã„ã¨**: 10ä¸‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚ã« **DBä¿å­˜ã‚¨ãƒ©ãƒ¼** ã§ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ã®ãƒªã‚¹ã‚¯ ğŸš¨
